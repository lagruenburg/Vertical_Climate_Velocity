{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "brutal-queen",
   "metadata": {},
   "source": [
    "# Climate Velocity functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "least-deposit",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "from shapely.geometry import Polygon, Point, MultiPolygon\n",
    "import shapefile\n",
    "import geopandas as gpd\n",
    "import xarray as xr\n",
    "import time\n",
    "import numpy as np\n",
    "from scipy.stats import linregress\n",
    "from scipy.interpolate import interp1d as int1\n",
    "import matplotlib.pyplot as plt\n",
    "import getpass\n",
    "import pandas as pn\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beautiful-school",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login information for the Copernicus Marine\n",
    "USERNAME = 'your_username_here'\n",
    "PASSWORD = getpass.getpass('Enter your password: ')\n",
    "DATASET_ID = 'cmems_mod_glo_phy_my_0.083_P1M-m'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naked-thread",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to retrieve data from copernicus\n",
    "def copernicusmarine_datastore(dataset, username, password):\n",
    "    from pydap.client import open_url\n",
    "    from pydap.cas.get_cookies import setup_session\n",
    "    cas_url = 'https://cmems-cas.cls.fr/cas/login'\n",
    "    session = setup_session(cas_url, username, password)\n",
    "    session.cookies.set(\"CASTGC\", session.cookies.get_dict()['CASTGC'])\n",
    "    database = ['my', 'nrt']\n",
    "    url = f'https://{database[0]}.cmems-du.eu/thredds/dodsC/{dataset}'\n",
    "    try:\n",
    "        data_store = xr.backends.PydapDataStore(open_url(url, session=session))#, user_charset='utf-8')) # needs PyDAP >= v3.3.0 see https://github.com/pydap/pydap/pull/223/commits \n",
    "    except:\n",
    "        url = f'https://{database[1]}.cmems-du.eu/thredds/dodsC/{dataset}'\n",
    "        data_store = xr.backends.PydapDataStore(open_url(url, session=session))#, user_charset='utf-8')) # needs PyDAP >= v3.3.0 see https://github.com/pydap/pydap/pull/223/commits\n",
    "    return data_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "certified-nursing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve Data\n",
    "data_store = copernicusmarine_datastore(DATASET_ID, USERNAME, PASSWORD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "close-madison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at the dataset\n",
    "DS = xr.open_dataset(data_store)\n",
    "DS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "written-checkout",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the LME datafile\n",
    "LME = gpd.read_file('/Users/nyelab/Downloads/LME66/LMEs66.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wireless-domain",
   "metadata": {},
   "source": [
    "## Functions for calculating climate velocity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worst-laser",
   "metadata": {},
   "source": [
    "### The final function **climate_vel_subsets_more** includes all the functions listed below.  You input an LME and a filename and directory for your output files and vertical and horizontal climate velocities are calculated and saved to that directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incident-watch",
   "metadata": {},
   "source": [
    "**find_nearest** and **LME_data_retreval_2** are used to subset the temperature data to the LME of intereste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dated-picture",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nearest(array, value):\n",
    "    array = np.asarray(array)\n",
    "    idx = (np.abs(array - value)).argmin()\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "economic-winter",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LME_data_retreval_2(LME_name):\n",
    "    lme = LME[LME.LME_NAME  == LME_name]\n",
    "    bounds = lme.geometry.apply(lambda x: x.bounds).tolist()\n",
    "    lon_min=bounds[0][0]\n",
    "    lat_min=bounds[0][1]\n",
    "    lon_max=bounds[0][2]\n",
    "    lat_max=bounds[0][3]\n",
    "    if lon_min <-100 and lon_max > 100:\n",
    "        lon_min_ind = find_nearest(totalLON, lon_min) + 5\n",
    "        lon_max_ind = find_nearest(totalLON, lon_max) - 5\n",
    "    else:\n",
    "        lon_min_ind = find_nearest(totalLON, lon_min) - 5\n",
    "        lon_max_ind = find_nearest(totalLON, lon_max) + 5\n",
    "    if lat_min < -79.58332059999996:\n",
    "        lat_min_ind = 0\n",
    "        lat_max_ind = find_nearest(totalLAT, lat_max) + 5\n",
    "    if lat_max > 89.5885158000143:\n",
    "        lat_min_ind = find_nearest(totalLAT, lat_min) - 5\n",
    "        lat_max_ind = 2040\n",
    "    if lat_min >= -79.58332059999996 and lat_max <= 89.5885158000143:\n",
    "        lat_min_ind = find_nearest(totalLAT, lat_min) - 5\n",
    "        lat_max_ind = find_nearest(totalLAT, lat_max) + 5\n",
    "    data = DS.thetao[:,:27,lat_min_ind:lat_max_ind,lon_min_ind:lon_max_ind]\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demanding-plymouth",
   "metadata": {},
   "source": [
    "**inSHAPE_all_depth_glorys** replaces all temperature data outside of the LME with nans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collectible-nitrogen",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inSHAPE_all_depth_glorys(longitude, latitude, var, depth, shape):\n",
    "    x = longitude\n",
    "    y = latitude\n",
    "    empty = np.empty([28, len(depth), len(y), len(x)])\n",
    "    empty[:,:,:,:] = np.nan\n",
    "    for i in range(len(y)):\n",
    "        for j in range(len(x)):\n",
    "            if Point(x[j], y[i]).within(shape) == True:\n",
    "                empty[:,:,i,j] = var[:,:,i,j]\n",
    "    return empty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dental-casino",
   "metadata": {},
   "source": [
    "**interpolate_5m_2** linearly interpolates the temperature data at 5m spacing in the upper 200m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "streaming-alaska",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_5m_2(tempdata, LME_Name):\n",
    "    start = time.perf_counter()\n",
    "    \n",
    "    datayr = tempdata.groupby('time.year').mean('time')\n",
    "    \n",
    "    time2 = time.perf_counter()\n",
    "    print('Yearly averaging completed, '+ str((time2-start)/60) + ' minutes elapsed')\n",
    "    \n",
    "    temp = inSHAPE_all_depth_glorys(datayr.longitude, datayr.latitude, datayr.thetao, datayr.depth,\n",
    "                             LME[LME.LME_NAME == LME_Name].geometry[LME.OBJECTID[LME.LME_NAME == LME_Name].values[0] -1])\n",
    "    \n",
    "    time3 = time.perf_counter()\n",
    "    print('Subsetting to LME area completed, '+ str((time3-start)/60) + ' minutes elapsed')\n",
    "    \n",
    "    z = np.arange(5,205,5)\n",
    "    \n",
    "    temp_5m = np.empty([28,len(z),len(datayr.latitude),len(datayr.longitude)])\n",
    "    temp_5m[:,:,:,:] = np.nan\n",
    "    for t in range(28):\n",
    "        for i in range(len(datayr.latitude)):\n",
    "            for j in range(len(datayr.longitude)):\n",
    "                Y_data = temp[t,:,i,j]\n",
    "                if np.any(Y_data)==True:\n",
    "                    func = int1(datayr.depth,Y_data,bounds_error = False)\n",
    "                    temp_5m[t,:,i,j] = func(z)\n",
    "                    \n",
    "    time4 = time.perf_counter()\n",
    "    print('5m interpolation completed, '+ str((time4-start)/60) + ' minutes elapsed')\n",
    "    \n",
    "    temp5m = xr.Dataset(data_vars = {'temp': (['year','depth','lat','lon'],temp_5m)}, \n",
    "                    coords = {'year': datayr.year, 'depth': z, 'lat': np.array(datayr.latitude), 'lon': np.array(datayr.longitude)})\n",
    "    return temp5m\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yellow-queue",
   "metadata": {},
   "source": [
    "**temptrend_vcv** calculates the slope and pvals of any significant linear trend of temperature at each gridpoint within the LME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "selected-completion",
   "metadata": {},
   "outputs": [],
   "source": [
    "def temptrend_vcv(temp5m):\n",
    "    \n",
    "    start = time.perf_counter()\n",
    "    \n",
    "    slopes = np.empty([len(temp5m.lat),len(temp5m.lon),len(temp5m.depth)])\n",
    "    p_vals = np.empty([len(temp5m.lat),len(temp5m.lon),len(temp5m.depth)])\n",
    "    slopes[:,:,:] = np.nan\n",
    "    p_vals[:,:,:] = np.nan\n",
    "\n",
    "    x = np.arange(0,27)\n",
    "    for i in range(len(temp5m.lat)):\n",
    "        for j in range(len(temp5m.lon)):\n",
    "            for k in range(len(temp5m.depth)):\n",
    "                y = temp5m.temp[:27,k,i,j]\n",
    "                if any(y) == True:\n",
    "                    LR = linregress(x, y)\n",
    "                    slopes[i,j,k] = LR.slope\n",
    "                    p_vals[i,j,k] = LR.pvalue\n",
    "    p_vals[p_vals>0.05] = np.nan                \n",
    "    time4 = time.perf_counter()\n",
    "    \n",
    "    print('Temperature Trends Complete, '+ str((time4-start)/60) + ' minutes elapsed')\n",
    "    \n",
    "    return slopes, p_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ranging-knock",
   "metadata": {},
   "source": [
    "**vertical_spat_grad** calculates the vertical spatial gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incredible-portrait",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vertical_spat_grad(temp_data):\n",
    "# Calculate the spatial gradient in the vertical\n",
    "    start = time.perf_counter()\n",
    "    \n",
    "    UP = 5 # 5 meters between each data point\n",
    "    DOWN = 5\n",
    "\n",
    "    temp_slice_mean = np.nanmean(temp_data.temp[:,:,:,:],0)\n",
    "    UD_net = np.empty([len(temp_data.depth),len(temp_data.lat),len(temp_data.lon)])\n",
    "    UD_net[:,:,:] = np.nan\n",
    "\n",
    "    for i in range(len(temp_data.depth)):\n",
    "        for j in range(len(temp_data.lat)):\n",
    "            for k in range(len(temp_data.lon)):\n",
    "                if np.isnan(temp_slice_mean[i,j,k]) == False:\n",
    "                    if i == 0: #at the surface\n",
    "                        focal_temp = temp_slice_mean[i,j,k]\n",
    "                        DO_grad = -1*(focal_temp - temp_slice_mean[i+1,j,k])/DOWN # angle = 0\n",
    "                        UD_net[i,j,k] = DO_grad\n",
    "                    if i == (len(temp_data.depth) -1): #at the bottom\n",
    "                        focal_temp = temp_slice_mean[i,j,k]\n",
    "                        UP_grad = (focal_temp - temp_slice_mean[i-1,j,k])/UP # angle = 180\n",
    "                        UD_net[i,j,k] = UP_grad\n",
    "                    if i >0 and i < (len(temp_data.depth)-1):\n",
    "                        focal_temp = temp_slice_mean[i,j,k]\n",
    "                        UP_grad = (focal_temp - temp_slice_mean[i-1,j,k])/UP # angle = 0\n",
    "                        DO_grad = -1*((focal_temp - temp_slice_mean[i+1,j,k])/DOWN) # angle = 180\n",
    "                        if np.isnan(UP_grad) == True and np.isnan(DO_grad) == True:\n",
    "                            UD_net[i,j,k] = np.nan\n",
    "                        else:\n",
    "                            UD_net[i,j,k] = np.nansum([UP_grad, DO_grad])/2\n",
    "\n",
    "    time4 = time.perf_counter()\n",
    "    \n",
    "    print('Vertical Spat Grad Complete, '+ str((time4-start)/60) + ' minutes elapsed')\n",
    "    \n",
    "    return UD_net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statistical-dividend",
   "metadata": {},
   "source": [
    "**temptrend_hcv** calculates the slope and pvals of any significant linear trend of temperature at each gridpoint within the LME using only the original uppermost bin (closest to the surface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "important-poland",
   "metadata": {},
   "outputs": [],
   "source": [
    "def temptrend_hcv(temp):\n",
    "    \n",
    "    slopes = np.empty([len(temp.lat),len(temp.lon)])\n",
    "    p_vals = np.empty([len(temp.lat),len(temp.lon)])\n",
    "    slopes[:,:] = np.nan\n",
    "    p_vals[:,:] = np.nan\n",
    "\n",
    "    x = np.arange(0,27)\n",
    "    for i in range(len(temp.lat)):\n",
    "        for j in range(len(temp.lon)):\n",
    "            y = temp.temp[:27,i,j]\n",
    "            if any(y) == True:\n",
    "                LR = linregress(x, y)\n",
    "                slopes[i,j] = LR.slope\n",
    "                p_vals[i,j] = LR.pvalue\n",
    "                    \n",
    "    p_vals[p_vals>0.05] = np.nan\n",
    "    return slopes, p_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "still-serve",
   "metadata": {},
   "source": [
    "**horiz_spatgrad_hcv** calculates the horizontal spatial gradient in the method of Burrows et al. (2011)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abandoned-congo",
   "metadata": {},
   "outputs": [],
   "source": [
    "def horiz_spatgrad_hcv(netcdf):\n",
    "\n",
    "    slice_mean = np.nanmean(netcdf.temp[:,:,:],0)\n",
    "    shape = np.shape(slice_mean)\n",
    "    spatgrad_NS = np.empty(shape)\n",
    "    spatgrad_NS[:,:] = np.nan\n",
    "    spatgrad_EW = np.empty(shape)\n",
    "    spatgrad_EW[:,:] = np.nan\n",
    "    spatgrad = np.empty(shape)\n",
    "    spatgrad[:,:] = np.nan\n",
    "\n",
    "    for j in range(len(netcdf.lat)):\n",
    "        for k in range(len(netcdf.lon)):\n",
    "            if np.isnan(slice_mean[j,k]) == False:\n",
    "                if j==0:\n",
    "                    spatgrad_NS[j,k] = np.nan\n",
    "                    spatgrad_EW[j,k] = np.nan\n",
    "                if j==(len(netcdf.lat)-1):\n",
    "                    spatgrad_NS[j,k] = np.nan\n",
    "                    spatgrad_EW[j,k] = np.nan\n",
    "                if k==0:\n",
    "                    spatgrad_NS[j,k] = np.nan\n",
    "                    spatgrad_EW[j,k] = np.nan\n",
    "                if k==(len(netcdf.lon)-1):\n",
    "                    spatgrad_NS[j,k] = np.nan\n",
    "                    spatgrad_EW[j,k] = np.nan\n",
    "                    \n",
    "                focal_temp = slice_mean[j,k]\n",
    "                if j > 0 and j < (len(netcdf.lat)-1) and k >0 and k< (len(netcdf.lon)-1):\n",
    "                    north_temp = slice_mean[j+1,k]\n",
    "                    south_temp = slice_mean[j-1,k]\n",
    "                    east_temp = slice_mean[j,k+1]\n",
    "                    west_temp = slice_mean[j,k-1]\n",
    "                    north_east_temp = slice_mean[j+1, k+1]\n",
    "                    north_west_temp = slice_mean[j+1, k-1]\n",
    "                    south_east_temp = slice_mean[j-1, k+1]\n",
    "                    south_west_temp = slice_mean[j-1, k-1]\n",
    "                \n",
    "                    gradWE1 = (north_temp - north_west_temp)/((np.cos(np.radians(netcdf.lat[j+1]))*111.321)*(1/12))\n",
    "                    gradWE2 = (focal_temp-west_temp)/((np.cos(np.radians(netcdf.lat[j]))*111.321)*(1/12))\n",
    "                    gradWE3 = (south_temp - south_west_temp)/((np.cos(np.radians(netcdf.lat[j-1]))*111.321)*(1/12))\n",
    "                    gradWE4 = (north_east_temp - north_temp)/((np.cos(np.radians(netcdf.lat[j+1]))*111.321)*(1/12))\n",
    "                    gradWE5 = (east_temp-focal_temp)/((np.cos(np.radians(netcdf.lat[j]))*111.321)*(1/12))\n",
    "                    gradWE6 = (south_east_temp - south_temp)/((np.cos(np.radians(netcdf.lat[j-1]))*111.321)*(1/12))\n",
    "                \n",
    "                    gradNS1 = (north_west_temp-west_temp)/(111.2*1/12)\n",
    "                    gradNS2 = (north_temp-focal_temp)/(111.2*1/12)\n",
    "                    gradNS3 = (north_east_temp - east_temp)/(111.2*1/12)\n",
    "                    gradNS4 = (west_temp - south_west_temp)/(111.2*1/12)\n",
    "                    gradNS5 = (focal_temp-south_temp)/(111.2*1/12)\n",
    "                    gradNS6 = (east_temp - south_east_temp)/(111.2*1/12)\n",
    "            \n",
    "                    spatgrad_EW[j,k] = np.average([gradWE1,gradWE2,gradWE3,gradWE4,gradWE5,gradWE6], weights = [1,2,1,1,2,1])\n",
    "                    spatgrad_NS[j,k] = np.average([gradNS1,gradNS2,gradNS3,gradNS4,gradNS5,gradNS6], weights = [1,2,1,1,2,1])\n",
    "                \n",
    "                    spatgrad[j,k] = np.sqrt((spatgrad_EW[j,k]**2)+(spatgrad_NS[j,k]**2))\n",
    "\n",
    "    return spatgrad_EW, spatgrad_NS, spatgrad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finished-grain",
   "metadata": {},
   "source": [
    "**VCV_HCV** combines the outputs of all the previous functions and returns xr datasets with all the vertical and horizontal climate velocity data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mobile-detail",
   "metadata": {},
   "outputs": [],
   "source": [
    "def VCV_HCV(VSG,TT5m,PV5m,temp5m, temp_yr, TT_yr, PV_yr, sg, ew_sg, ns_sg):\n",
    "    \n",
    "    vcv = xr.Dataset(data_vars = {'temp': (['year','depth','lat','lon'],temp5m.temp),\n",
    "                                 'v_spatgrad': (['depth','lat','lon'], VSG),\n",
    "                                 'temptrend': (['lat','lon','depth'], TT5m),\n",
    "                                  'pvals': (['lat','lon','depth'], PV5m),\n",
    "                                 'VCV': (['lat','lon','depth'],TT5m/(VSG.transpose(1,2,0)))},\n",
    "                     coords = {'year': temp5m.year, 'depth': temp5m.depth, \n",
    "                               'lat': np.array(temp5m.lat), \n",
    "                               'lon': np.array(temp5m.lon)})\n",
    "    \n",
    "    hcv = xr.Dataset(data_vars = {'temp': (['year','lat','lon'],temp_yr.temp),\n",
    "                                  'ew_spatgrad': (['lat','lon'], ew_sg),\n",
    "                                  'ns_spatgrad': (['lat','lon'], ns_sg),\n",
    "                                  'horiz_spatgrad': (['lat','lon'],sg),\n",
    "                                  'temptrend': (['lat','lon'], TT_yr),\n",
    "                                  'pvals': (['lat','lon'], PV_yr),\n",
    "                                  'HCV': (['lat','lon'], TT_yr/sg)},\n",
    "                     coords = {'year': temp_yr.year, \n",
    "                               'lat': np.array(temp_yr.lat), \n",
    "                               'lon': np.array(temp_yr.lon)})\n",
    "    return vcv, hcv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outdoor-second",
   "metadata": {},
   "source": [
    "**climate_vel_subsets_more** uses all previous functions to create and save vertical and horizontal climate velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extra-institution",
   "metadata": {},
   "outputs": [],
   "source": [
    "def climate_vel_subsets_more(LME_name, file_name, directory):\n",
    "    \n",
    "    data = LME_data_retreval_2(LME_name)\n",
    "    location = directory + file_name + '1.nc'\n",
    "    data[:,:3,:,:].to_netcdf(location)\n",
    "    \n",
    "    data = LME_data_retreval_2(LME_name)\n",
    "    location = directory + file_name + '2.nc'\n",
    "    data[:,3:6,:,:].to_netcdf(location)\n",
    "    \n",
    "    data = LME_data_retreval_2(LME_name)\n",
    "    location = directory + file_name + '3.nc'\n",
    "    data[:,6:9,:,:].to_netcdf(location)\n",
    "    \n",
    "    data = LME_data_retreval_2(LME_name)\n",
    "    location = directory + file_name + '4.nc'\n",
    "    data[:,9:12,:,:].to_netcdf(location)\n",
    "    \n",
    "    data = LME_data_retreval_2(LME_name)\n",
    "    location = directory + file_name + '5.nc'\n",
    "    data[:,12:15,:,:].to_netcdf(location)\n",
    "    \n",
    "    data = LME_data_retreval_2(LME_name)\n",
    "    location = directory + file_name + '6.nc'\n",
    "    data[:,15:18,:,:].to_netcdf(location)\n",
    "    \n",
    "    data = LME_data_retreval_2(LME_name)\n",
    "    location = directory + file_name + '7.nc'\n",
    "    data[:,18:21,:,:].to_netcdf(location)\n",
    "    \n",
    "    data = LME_data_retreval_2(LME_name)\n",
    "    location = directory + file_name + '8.nc'\n",
    "    data[:,21:,:,:].to_netcdf(location)\n",
    "    \n",
    "    data1 = xr.open_dataset(directory + file_name + '1.nc')\n",
    "    data2 = xr.open_dataset(directory + file_name + '2.nc')\n",
    "    data3 = xr.open_dataset(directory + file_name + '3.nc')\n",
    "    data4 = xr.open_dataset(directory + file_name + '4.nc')\n",
    "    data5 = xr.open_dataset(directory + file_name + '5.nc')\n",
    "    data6 = xr.open_dataset(directory + file_name + '6.nc')\n",
    "    data7 = xr.open_dataset(directory + file_name + '7.nc')\n",
    "    data8 = xr.open_dataset(directory + file_name + '8.nc')\n",
    "    \n",
    "    data = xr.merge([data1,data2,data3,data4,data5,data6,data7,data8])\n",
    "    \n",
    "    LME_Name = LME_name\n",
    "    # Vertical Climate Velocity\n",
    "    # interpolate to 5m\n",
    "    data5m = interpolate_5m_2(data, LME_name)\n",
    "    \n",
    "    #calculate the temperature trend\n",
    "    slopes, pvals = temptrend_vcv(data5m)\n",
    "    \n",
    "    #calculate the spatial gradient\n",
    "    spatgrad_vert = vertical_spat_grad(data5m)\n",
    "    \n",
    "    # Horizontal Climate Velocity\n",
    "    #average to yearly timestep\n",
    "    data_year = data.groupby('time.year').mean('time')\n",
    "    \n",
    "    data_yr = inSHAPE_all_depth_glorys(data_year.longitude, data_year.latitude, data_year.thetao[:,:2,:,:], data_year.depth[:2],\n",
    "                             LME[LME.LME_NAME == LME_Name].geometry[LME.OBJECTID[LME.LME_NAME == LME_Name].values[0] -1])\n",
    "    datayr = xr.Dataset(data_vars = {'temp': (['year','lat','lon'],data_yr[:,0,:,:])}, \n",
    "                   coords = {'year': data_year.year, 'lat': np.array(data_year.latitude), 'lon': np.array(data_year.longitude)})\n",
    "    \n",
    "    #calculate the temperature trend\n",
    "    s, p = temptrend_hcv(datayr)\n",
    "    \n",
    "    #calculate the horizontal spatial gradient\n",
    "    ew_sg, ns_sg, sg = horiz_spatgrad_hcv(datayr)\n",
    "    \n",
    "    #calculate the climate velocity\n",
    "    vcv, hcv = VCV_HCV(spatgrad_vert,slopes,pvals,data5m, datayr, s, p, sg, ew_sg, ns_sg)\n",
    "    \n",
    "    vcv.to_netcdf(directory + file_name + 'vcv.nc')\n",
    "    hcv.to_netcdf(directory + file_name + 'hcv.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "personal-discretion",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
